{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import pickle\n",
    "\n",
    "from numpy import nan\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.0 Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data into data frame\n",
    "data = pd.read_csv('Resources/dataset.csv',header = 0, names = [\"tweet_id\", \"sentiment\", \"author\", \"content\"], usecols = [\"sentiment\", \"content\"])\n",
    "\n",
    "\n",
    "#Clean the missing data\n",
    "count = 0\n",
    "for line in data.content:\n",
    "    if line in ['0', nan]:\n",
    "        data = data.drop(data.index[count])\n",
    "        count = count - 1\n",
    "    count = count + 1\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# print(tabulate(data.head(10), showindex=True, headers=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Save the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_1.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_1.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Replace the emoticons with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace the emoticons\n",
    "#Smile\n",
    "data.content = data.content.str.replace(r':\\)', 'happy')\n",
    "data.content = data.content.str.replace(r':-\\)', 'happy')\n",
    "data.content = data.content.str.replace(r'=\\)', 'happy')\n",
    "data.content = data.content.str.replace(r':]', 'happy')\n",
    "data.content = data.content.str.replace(r'=]', 'happy')\n",
    "\n",
    "#Sad\n",
    "data.content = data.content.str.replace(r':\\(', 'sad')\n",
    "data.content = data.content.str.replace(r':-\\(', 'sad')\n",
    "data.content = data.content.str.replace(r'=\\(', 'sad')\n",
    "data.content = data.content.str.replace(r':\\[', 'sad')\n",
    "data.content = data.content.str.replace(r'=\\[', 'sad')\n",
    "\n",
    "#Surprise\n",
    "data.content = data.content.str.replace(r':-O', 'surprise')\n",
    "\n",
    "#Angry\n",
    "data.content = data.content.str.replace(r':-@', 'angry')\n",
    "\n",
    "#Confused\n",
    "data.content = data.content.str.replace(r':-$', 'confused')\n",
    "\n",
    "#Secret\n",
    "data.content = data.content.str.replace(r':-#', 'secret')\n",
    "\n",
    "#Rolling eyes\n",
    "data.content = data.content.str.replace(r'@@', 'rolling eyes')\n",
    "\n",
    "#Laughing\n",
    "data.content = data.content.str.replace(r':-D', 'laughing')\n",
    "\n",
    "#Winking smile\n",
    "data.content = data.content.str.replace(r';\\)', '')\n",
    "data.content = data.content.str.replace(r';-\\)', '')\n",
    "\n",
    "#Happy crying\n",
    "data.content = data.content.str.replace(r\":'\\)\", 'happy crying')\n",
    "data.content = data.content.str.replace(r\":'-\\)\", 'happy crying')\n",
    "\n",
    "#Smile with tongue hanging out\n",
    "data.content = data.content.str.replace(r':p', 'smile with tongue hanging out')\n",
    "\n",
    "#Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_2.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_2.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()\n",
    "\n",
    "\n",
    "# print(tabulate(data.head(26), showindex=True, headers=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Text Processing :\n",
    "1. Remove hashtag but remain the words\n",
    "2. Remove Twitter username mention\n",
    "3. Remove URL\n",
    "4. Remove &amp\n",
    "5. Convert the letter to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove hashtag but remain the words\n",
    "data.content = data.content.str.replace(r'#', '')\n",
    "\n",
    "#Remove Twitter username mention\n",
    "data.content = data.content.str.replace(r'@\\S+', '')\n",
    "\n",
    "#Remove URL\n",
    "data.content = data.content.str.replace(r'http\\S+', 'website')\n",
    "\n",
    "#Remove &amp\n",
    "data.content = data.content.str.replace(r'&amp\\S+', '')\n",
    "\n",
    "#Convert the letter to lower case\n",
    "data.content = data.content.str.lower()\n",
    "\n",
    "#Clean the missing data\n",
    "count = 0\n",
    "for line in data.content:\n",
    "    if line in ['0', nan,'']:\n",
    "        data = data.drop(data.index[count])\n",
    "        count = count - 1\n",
    "    count = count + 1\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_3.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_3.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()\n",
    "\n",
    "# print(tabulate(data.head(10), showindex=True, headers=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Remove the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Limit the punctuation\n",
    "data.content = data.content.str.replace(r'[^\\w\\s]', '')\n",
    "\n",
    "#Clean the missing data\n",
    "count = 0\n",
    "for line in data.content:\n",
    "    if line in ['0', nan,'']:\n",
    "        data = data.drop(data.index[count])\n",
    "        count = count - 1\n",
    "    count = count + 1\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "##Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_4.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_4.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()\n",
    "\n",
    "# print(tabulate(data.head(10), showindex=True, headers=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Reconstruct the abbrevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reconstruct the abbrevations\n",
    "with open('Resources/Abbreviations.csv', mode = 'r') as infile:\n",
    "    lower_stream = (line.lower() for line in infile)\n",
    "    reader = csv.reader(lower_stream)\n",
    "    mydict = {rows[0]:rows[1] for rows in reader}\n",
    "#     print(mydict)\n",
    "    \n",
    "def process(dat):\n",
    "    count = 0\n",
    "    dat = dat.str.lower()\n",
    "    for line in dat:\n",
    "        da = ''.join( mydict.get( word, word ) for word in re.split( '(\\W+)', str(line) ) )\n",
    "        data.content[count] = da\n",
    "        count = count + 1\n",
    "\n",
    "process(data.content)\n",
    "\n",
    "##Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_5.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_5.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()\n",
    "\n",
    "# print(tabulate(data.head(10), showindex=True, headers=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "# A set of stop words to filter the filler words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)\n",
    "\n",
    "for num in range(len(data.content)):\n",
    "    word_tokens = word_tokenize(data.content[num])\n",
    "    data.content[num] = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    \n",
    "#Clean the missing data\n",
    "count = 0\n",
    "for line in data.content:\n",
    "    if line in ['0', nan]:\n",
    "        print(line)\n",
    "        data = data.drop(data.index[count])\n",
    "        count = count - 1\n",
    "    count = count + 1\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "##Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_6.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_6.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()\n",
    "\n",
    "# print(tabulate(data.head(10), showindex=True, headers=data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Word Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Correct the words\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the function\n",
    "count = 0\n",
    "for line in data.content:\n",
    "    temp =[]\n",
    "    for word in line:\n",
    "            temp.append(correction(word))\n",
    "    data.content[count] = temp\n",
    "    count = count + 1\n",
    "\n",
    "#Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_7.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_7.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemming the words\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for num in range(len(data.content)):\n",
    "    data.content[num] = [ps.stem(word) for word in data.content[num]]\n",
    "\n",
    "#Save the processed data into csv\n",
    "data.to_csv('Outputs\\csv\\preprocessed_data_8.csv', index = False)\n",
    "\n",
    "#Save the file in pickle format\n",
    "outputs = open('Outputs\\pkl\\preprocessed_data_8.pkl','wb')\n",
    "pickle.dump(data, outputs)\n",
    "outputs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
